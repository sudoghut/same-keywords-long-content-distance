{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same Keywords Long Distance Analysis\n",
    "\n",
    "This notebook analyzes news data with the same keywords to find pairs with the lowest similarity scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "import os\n",
    "from typing import List, Union\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Read Database Path from .env File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_db_path_from_env(env_file: str = '.env') -> str:\n",
    "    \"\"\"\n",
    "    Read database path from .env file.\n",
    "    Expected format: db_path: xxx\n",
    "    \n",
    "    Args:\n",
    "        env_file: Path to the .env file\n",
    "        \n",
    "    Returns:\n",
    "        Database path as string\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(env_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if line.startswith('db_path:'):\n",
    "                    db_path = line.split('db_path:')[1].strip()\n",
    "                    return db_path\n",
    "        raise ValueError(\"db_path not found in .env file\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"{env_file} file not found\")\n",
    "\n",
    "# Read database path\n",
    "db_path = read_db_path_from_env()\n",
    "print(f\"Database path: {db_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Query Database and Create Initial Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_news_data(db_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Query the database to get news data with keywords.\n",
    "    Joins main_news_data with serpapi_data to get the query field.\n",
    "    \n",
    "    Args:\n",
    "        db_path: Path to the SQLite database\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with id, serpapi_id, news, date, and keywords columns\n",
    "    \"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_path)\n",
    "        \n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            m.id,\n",
    "            m.serpapi_id,\n",
    "            m.news,\n",
    "            m.date,\n",
    "            s.query as keywords\n",
    "        FROM main_news_data m\n",
    "        JOIN serpapi_data s ON m.serpapi_id = s.id\n",
    "        WHERE m.news IS NOT NULL AND m.news != ''\n",
    "        ORDER BY s.query, m.date\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        conn.close()\n",
    "        \n",
    "        # Convert date to datetime\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        print(f\"Total records loaded: {len(df)}\")\n",
    "        print(f\"Unique keywords: {df['keywords'].nunique()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Error querying database: {str(e)}\")\n",
    "\n",
    "# Load data\n",
    "df = query_news_data(db_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Modular Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the embedding model (global variable for efficiency)\n",
    "embedding_model = None\n",
    "\n",
    "def get_embeddings(texts: Union[str, List[str]], model_name: str = \"Qwen/Qwen3-Embedding-0.6B\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate embeddings for input text(s) using the specified model.\n",
    "    This function is modular and can be easily replaced with different embedding models.\n",
    "    \n",
    "    Args:\n",
    "        texts: Single text string or list of text strings to embed\n",
    "        model_name: Name of the sentence transformer model to use\n",
    "        \n",
    "    Returns:\n",
    "        numpy array of embeddings\n",
    "    \"\"\"\n",
    "    global embedding_model\n",
    "    \n",
    "    # Load model only once\n",
    "    if embedding_model is None:\n",
    "        print(f\"Loading embedding model: {model_name}...\")\n",
    "        embedding_model = SentenceTransformer(model_name)\n",
    "        print(\"Model loaded successfully!\")\n",
    "    \n",
    "    # Convert single string to list\n",
    "    if isinstance(texts, str):\n",
    "        texts = [texts]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "def calculate_cosine_similarity(embedding1: np.ndarray, embedding2: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Calculate cosine similarity between two embeddings.\n",
    "    \n",
    "    Args:\n",
    "        embedding1: First embedding vector\n",
    "        embedding2: Second embedding vector\n",
    "        \n",
    "    Returns:\n",
    "        Cosine similarity score\n",
    "    \"\"\"\n",
    "    return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n",
    "\n",
    "# Test the embedding function with a sample\n",
    "print(\"Testing embedding function...\")\n",
    "test_embedding = get_embeddings(\"This is a test sentence.\")\n",
    "print(f\"Embedding shape: {test_embedding.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Find Lowest Similarity Pairs for Each Keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lowest_similarity_pairs(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each keyword with multiple records, find the pair with the lowest similarity.\n",
    "    The pair consists of later news (news1) and earlier news (news2) based on date.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with news data\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with lowest similarity pairs\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Group by keywords\n",
    "    grouped = df.groupby('keywords')\n",
    "\n",
    "    # Filter out groups with only one record\n",
    "    df_filtered = df[df.groupby('keywords')['keywords'].transform('size') > 1]\n",
    "    grouped = df_filtered.groupby('keywords')\n",
    "\n",
    "    print(f\"Processing {len(grouped)} keyword groups...\")\n",
    "\n",
    "    \n",
    "    for keyword, group in grouped:\n",
    "        # Skip keywords with only one record\n",
    "        if len(group) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Sort by date to ensure proper ordering\n",
    "        group = group.sort_values('date').reset_index(drop=True)\n",
    "        \n",
    "        # Get embeddings for all news in this group\n",
    "        news_texts = group['news'].tolist()\n",
    "        embeddings = get_embeddings(news_texts)\n",
    "        \n",
    "        # Find the pair with lowest similarity\n",
    "        # Only consider pairs where news1 is later than news2\n",
    "        min_similarity = float('inf')\n",
    "        best_pair = None\n",
    "        \n",
    "        for i in range(len(group)):\n",
    "            for j in range(i):\n",
    "                # i is later (news1), j is earlier (news2)\n",
    "                similarity = calculate_cosine_similarity(embeddings[i], embeddings[j])\n",
    "                \n",
    "                if similarity < min_similarity:\n",
    "                    min_similarity = similarity\n",
    "                    best_pair = (i, j)\n",
    "        \n",
    "        if best_pair is not None:\n",
    "            idx1, idx2 = best_pair\n",
    "            row1 = group.iloc[idx1]\n",
    "            row2 = group.iloc[idx2]\n",
    "            \n",
    "            # Calculate date difference\n",
    "            date_diff = (row1['date'] - row2['date']).days\n",
    "            \n",
    "            results.append({\n",
    "                'id': row1['id'],\n",
    "                'keywords': keyword,\n",
    "                'news1': row1['news'],\n",
    "                'news2': row2['news'],\n",
    "                'similarity': min_similarity,\n",
    "                'serpapi_id': row1['serpapi_id'],\n",
    "                'date_diff': date_diff,\n",
    "                'date1': row1['date'],\n",
    "                'date2': row2['date']\n",
    "            })\n",
    "    \n",
    "    # Create DataFrame from results\n",
    "    result_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Sort by similarity (lowest first)\n",
    "    result_df = result_df.sort_values('similarity').reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Found {len(result_df)} keyword pairs with lowest similarity\")\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "# Find lowest similarity pairs\n",
    "result_df = find_lowest_similarity_pairs(df)\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Display Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Summary Statistics ===\")\n",
    "print(f\"Total pairs found: {len(result_df)}\")\n",
    "print(f\"\\nSimilarity score statistics:\")\n",
    "print(result_df['similarity'].describe())\n",
    "print(f\"\\nDate difference statistics (days):\")\n",
    "print(result_df['date_diff'].describe())\n",
    "print(f\"\\nLowest similarity score: {result_df['similarity'].min():.4f}\")\n",
    "print(f\"Highest similarity score: {result_df['similarity'].max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(df: pd.DataFrame, filename: str = 'same_keywords_lowest_similarity.csv'):\n",
    "    \"\"\"\n",
    "    Save the DataFrame to a CSV file with UTF-8 with BOM encoding.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        filename: Output filename\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"\\nResults saved to: {filename}\")\n",
    "        print(f\"Total rows saved: {len(df)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving CSV: {str(e)}\")\n",
    "\n",
    "# Save results\n",
    "save_to_csv(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Display Sample Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Sample Results (Top 5 Lowest Similarity Pairs) ===\")\n",
    "for idx, row in result_df.head(5).iterrows():\n",
    "    print(f\"\\n--- Pair {idx + 1} ---\")\n",
    "    print(f\"Keywords: {row['keywords']}\")\n",
    "    print(f\"Similarity: {row['similarity']:.4f}\")\n",
    "    print(f\"Date difference: {row['date_diff']} days\")\n",
    "    print(f\"Date1: {row['date1']}\")\n",
    "    print(f\"Date2: {row['date2']}\")\n",
    "    print(f\"News1 (later): {row['news1'][:200]}...\")\n",
    "    print(f\"News2 (earlier): {row['news2'][:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
